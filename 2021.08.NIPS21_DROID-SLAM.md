# DROID-SLAM

Conference: NIPS21
Date of first release: 2021.08

### RAFT: Recurrent All-Pairs Field Transform

(ECCV-2020 Best paper award), **Supervised**

- To create a **measure of visual similarity,**
    - they are finding **All-Pairs Correlation**. It is a 4D correlation volume, since every pixel of one image is correlated with every pixel in the second image.
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled.png)
    
    But not doing for raw image, but for the features of the image (at 1/8th resolution of the actual img)
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%201.png)
    
    - doing optical flow refinements, by looking at the current estimate of the optical flow, context features and Lookup from the correlation volume.
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%202.png)
    
    - so whole n/w is working at the 1/8th resolution.
    
- **How to do Lookup from correlation volume? :** Make a Correlation Pyramid by Avg pool last two diamensions of 4D volume.
    
    which smears out the high confidence activations of the 4D volume.
    
       
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%203.png)
    
    - Sampling from high resolution means it corresponds to small displacements (less motion), Sampling from coercer resolution means it corresponds to large displacements (less motion) and make large corrections to the optical flow.
        
        After some updates, we have to make just small updates, so we will be using only first(high res) level of pyramid.
        
    
- They use Conv GRU update operator to mimic iterative updates of a first order optimization algorithm.
    
    So, correlation features are used to do the update instead of gradients.
    
    **RAFT is Supervised** 
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%204.png)
    
- **Trained** on Synthetic Datasets. then **fine tuned** on Real life datasets(kitti , sintel)
- Visually, there optical flow for a video was temporally consistent.
- storing the full correlation volume eats up a lot of GPU memory. But computation wise it is just 20%. But they have a low memory implementation also where they store correl volume at coercer resolution.

### RAFT-Stereo

(3DV 2021)

Stereo matching is 1D analog of Optical Flow, And Scene flow is 3D analog of Optical flow.

Here, we will just have a 3D correlation volume, because for a pixel we have to take the correlation with only a horizontal line(not all pixels) in the other image.

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%205.png)

Their pcd from the predicted depth is very good.

## DROID-SLAM

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%206.png)

We can find optical flow if we have depth and camera poses:

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%207.png)

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%208.png)

So here: given the **optical flow**(using RAFT), we want to compute the **depth and camera poses.**

> DBA: find pixelwise depth and camera poses that are consistent with predicted flow(RAFT).
> 

Covisibility graph: edges b/w the frames that have some visual overlap.

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%209.png)

 

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2010.png)

- flow revisions are the update to predicted flow(RAFT output)

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2011.png)

this is a least square optimisation problem, we can linearize it and solve it.

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2012.png)

- Supervised on pose loss and flow loss.
- Trained on Monocular. No retraining is required for Stereo and RGB-D.

From their trajectory results from TartanAir dataset, if the trajectory has loop closures, then they have near to 0 drift, but if no loop closure is there, then a little drift can be seen.

- Results
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2013.png)
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2014.png)
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2015.png)
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2016.png)
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2017.png)
    

### What problem is the author trying to solve?

Fully differentiable Realtime Visual SLAM system for Monocular videos (works on stereo and RGB-D also). 

### How are they better than the previous methods?

- **high Accuracy** (large improvements over prior work: both learning based as well as Classical methods),
- **High Robustness** (less catastrophic failures),
- **Strong cross-dataset generalization** (trained on monocular TartanAir, tested on all other be it stereo or rgbd also).

### Proposed Method :

General idea they have applied to solve for Optical flow, Stereo Matching, SLAM and Scene flow:

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2018.png)

### How do they do it?

### What have they not done?

### Key Take away for us:

READ later:

bundle adjustment, SFM, least square optimization