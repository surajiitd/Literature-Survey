# DROID-SLAM

Conference: NIPS21
Date of first release: 2021.08

### RAFT: Recurrent All-Pairs Field Transform
(ECCV-2020 Best Paper Award)

**Supervised on Optical flow**

- To create a **measure of visual similarity,**
    - they are finding **All-Pairs Correlation**. It is a 4D correlation volume, since every pixel of one image is correlated with every pixel in the second image.
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled.png)
    
    But instead of doing for the raw image, they are doing for a feature map of the image (at 1/8th resolution of the actual img)
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%201.png)
    
    - doing optical flow refinements, by looking at the current estimate of the optical flow, context features and Lookup from the correlation volume.
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%202.png)
    
    - so whole n/w is working at the 1/8th resolution.
    
- **How to do Lookup from correlation volume? :** Make a Correlation Pyramid by Avg pool last two diamensions of 4D volume.
    
    which smears out the high confidence activations of the 4D volume.
    
       
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%203.png)
    
    - Sampling from high resolution means it corresponds to small displacements (less motion), Sampling from coercer resolution means it corresponds to large displacements (less motion) and make large corrections to the optical flow.
        
        After some updates, we have to make just small updates, so we will be using only first(high res) level of pyramid.
        
    
- They use Conv GRU update operator to mimic iterative updates of a first order optimization algorithm.
    
    So, correlation features are used to do the update instead of gradients.
    
    **RAFT is Supervised** 
    
    ![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%204.png)
    
- **Trained** on Synthetic Datasets. then **fine tuned** on Real life datasets(kitti , sintel)
- Visually, there optical flow for a video was temporally consistent.
- storing the full correlation volume eats up a lot of GPU memory. But computation wise it is just 20%. But they have a low memory implementation also where they store correl volume at coercer resolution.

### RAFT-Stereo
(3DV 2021)

- **Stereo matching is 1D analog of Optical Flow**, And Scene flow is 3D analog of Optical flow.
- Here, we will just have a 3D correlation volume, because for a pixel we have to take the correlation with only a horizontal line(not all pixels) in the other image.

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%205.png)

Their pcd from the predicted depth is very good.

## DROID-SLAM
(NIPS-2021)

### What problem is the author trying to solve?

Fully differentiable Realtime Visual SLAM system for Monocular videos (works on stereo and RGB-D also). 

### How are they better than the previous methods?

- **high Accuracy** (large improvements over prior work: both learning based as well as Classical methods),
- **High Robustness** (less catastrophic failures),
- **Strong cross-dataset generalization** (trained on monocular TartanAir, tested on all other.. be it stereo or rgbd also).

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%206.png)

### **Proposed Method**

We can find optical flow if we have depth and camera poses:

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%207.png)

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%208.png)

So here: given the **optical flow**(using RAFT), we want to compute the **depth and camera poses.**

> DBA: **Find pixelwise depth and camera poses that are consistent with predicted flow(RAFT).**
> 

Covisibility graph: we have edges b/w the frames that have some **visual overlap.**

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%209.png)

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2010.png)

 

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2011.png)

- flow revisions are the update to predicted flow(RAFT output)

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2012.png)

this is a least square optimisation problem, we can linearize it and solve it.

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2013.png)

- Supervised on **pose loss** and **flow loss**.

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2014.png)

- Trained on Monocular. No retraining is required for Stereo and RGB-D.

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2015.png)

From their trajectory results from TartanAir dataset, if the trajectory has loop closures, then they have near to 0 drift, but if no loop closure is there, then a little drift can be seen.

### Results

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2016.png)

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2017.png)

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2018.png)

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2019.png)

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2020.png)

---

### Some details about paper :

General idea they have applied to solve for Optical flow, Stereo Matching, SLAM and Scene flow:

![Untitled](2021.08.NIPS21_DROID-SLAM/Untitled%2021.png)

- Supervised using pose loss and flow loss( b/w gt_flow AND predicted_flow(which is getting updated using flow revisions) ).
- During Inference, the depth and the camera poses will get iteratively updated as new frames are processed.

- **Initialisation**:
    - **Keyframe selection**: mean optical flow should be **> 16px**(estimated by applying one update iteration).
    - Make frame graph for first **12 keyframes**(mean optical flow> 16px).
    - **Covisible Keyframe selection(edge selection)** in covisibility graph:
        - Keyframes have **edges** if they are 3 timesteps apart.
    - Then do **10 update iterations**.
    

- **Frontend:**
    - **Keyframe selection**: mean optical flow should be **> 4px**(estimated by applying one update iteration).
    - **process** each keyframe and add it to the covisibility graph.
    - Make edges with **3 closest neighbours** as measured by mean optical flow.
    - The depth and the camera poses for each pair (i,j) will get iteratively updated as new frames are processed.
    - after each depth,camera pose update, update the covisibility graph.
    - After adding a keyframe, remove one keyframe (redundant one or the oldest one).
    - **So Frontend also have 12 frames always.**
    - **Covisible Keyframe selection(edge selection)** in covisibility graph:
        - mean optical flow should be **> 16px for frontend.**

- **Backend:**
    - Keep all keyframes accumulated till now from fronted.
    - **Covisible Keyframe selection(edge selection)** in covisibility graph:
        - mean optical flow should be **>22px for backend.**
    - For loop closures, In paper its mentioned "If the camera returns to a previously mapped region, we add long range connections in the graph to perform loop closure."

- For each image pair (i,j) in covisibility graph, make a 4D correlation volume from their feature maps.
    - make a dense **correspondence field(p_ij)**(which gives the corresponding pixel locations of image_i in image_j) of size HxWx2 .
        - Use of the correspondence field(p_ij):
            - To estimate the optical flow. i.e., p_ij - p_j
            - To index the correlation volume OR to do the lookup in the correl volume

- Memory Requirements:
    
    frontend = 8GB
    backend : TUM-RGBD ( 1 1 GB)
    EuRoC, TartanAIr, ETH-3D (24GB)